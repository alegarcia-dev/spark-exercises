{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63dfdfb3-b815-4117-a20e-8783a84743e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 11:52:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/20 11:52:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecb36e-8c81-41a8-94a7-bf0d6878c373",
   "metadata": {},
   "source": [
    "# Data Wrangling Exercises\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### 1\n",
    "\n",
    "Read the case, department, and source data into their own spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ec3c79-21aa-48d3-af7e-66aabd5eb29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read all three .csv files into dataframes\n",
    "\n",
    "case = spark.read.csv(\"case.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "department = spark.read.csv(\"dept.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "source = spark.read.csv(\"source.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc108e-31f5-4016-8eca-a1ccacca4eca",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "Let's see how writing to the local disk works in spark:\n",
    "\n",
    "- Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "- Inspect your folder structure. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cce3437-47a2-4861-a145-545f4a906e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the source dataframe into json and csv formats\n",
    "\n",
    "source.write.json('source_json', mode = 'overwrite')\n",
    "source.write.csv('source_csv', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43179d75-d575-4c9d-b9b3-acdf3cdf10ea",
   "metadata": {},
   "source": [
    "The files are stored in partitions inside a directory by the name given to the write methods. There is also a _SUCCESS file.\n",
    "\n",
    "### 3\n",
    "\n",
    "Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecfa3fc-4057-4cc5-9252-5853f5c1e0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- SLA_due_date: string (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the case dataframe\n",
    "\n",
    "case.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f5e64d-528e-472b-9c77-ba54838d0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------\n",
      " case_id              | 1014127332                           \n",
      " case_opened_date     | 1/1/18 0:42                          \n",
      " case_closed_date     | 1/1/18 12:29                         \n",
      " SLA_due_date         | 9/26/20 0:42                         \n",
      " case_late            | NO                                   \n",
      " num_days_late        | -998.5087616000001                   \n",
      " case_closed          | YES                                  \n",
      " dept_division        | Field Operations                     \n",
      " service_request_type | Stray Animal                         \n",
      " SLA_days             | 999.0                                \n",
      " case_status          | Closed                               \n",
      " source_id            | svcCRMLS                             \n",
      " request_address      | 2315  EL PASO ST, San Antonio, 78207 \n",
      " council_district     | 5                                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case.show(1, vertical = True, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3e97b-620a-4bbc-a1c3-ca2420006ed3",
   "metadata": {},
   "source": [
    "The case_opened_date, case_closed_date, and SLA_due_date columns should be datetime types. The case_late and case_closed columns can be cast to boolean types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28d01c5-8ea8-4891-b22a-7b4891dce075",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"M/d/yy H:mm\"\n",
    "case = (\n",
    "    case.withColumn(\"case_opened_date\", F.to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", F.to_timestamp(\"case_closed_date\", fmt))\n",
    "    .withColumn(\"SLA_due_date\", F.to_timestamp(\"SLA_due_date\", fmt))\n",
    ")\n",
    "\n",
    "case = (\n",
    "    case.withColumn(\"case_closed\", F.expr('case_closed == \"YES\"'))\n",
    "    .withColumn(\"case_late\", F.expr('case_late == \"YES\"'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6406b1-ae54-4e96-abca-798d6a3c5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- standardized_dept_name: string (nullable = true)\n",
      " |-- dept_subject_to_SLA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the department dataframe\n",
    "\n",
    "department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fbf1eb-ff5a-4fc0-a4fe-76b7531fdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " dept_division          | 311 Call Center  \n",
      " dept_name              | Customer Service \n",
      " standardized_dept_name | Customer Service \n",
      " dept_subject_to_SLA    | YES              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department.show(1, vertical = True, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380dadb0-02f4-407e-a0bc-82d5fd3b78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|dept_subject_to_SLA|\n",
      "+-------------------+\n",
      "|                YES|\n",
      "|                 NO|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department.select('dept_subject_to_SLA').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a1cde-349a-4a86-929d-9515fe38b308",
   "metadata": {},
   "source": [
    "The dept_subject_to_SLA column can be a boolean type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bc9d65-a105-40e7-9611-dc795bc5be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "department = department.withColumn('dept_subject_to_SLA', F.expr('dept_subject_to_SLA == \"YES\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d346aa6e-b702-46a1-b2b9-8eadc0bec8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- source_username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally let's see the source dataframe\n",
    "\n",
    "source.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c5c46b-f56d-4d1a-801c-1c4aed62c808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " source_id       | 100137           \n",
      " source_username | Merlene Blodgett \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source.show(1, vertical = True, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be8bb4-fc58-4378-a61c-0d699f1ad4b9",
   "metadata": {},
   "source": [
    "The source_id column can probably be cast to an int type, but I don't think it is necessary.\n",
    "\n",
    "## Part 2\n",
    "\n",
    "### 1\n",
    "\n",
    "How old is the latest (in terms of days past SLA) currently open issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0303b8-5960-4f9f-a055-e484904ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this with SQL\n",
    "case.createOrReplaceTempView('case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b56730c6-4c6b-44eb-8c85-30df98b44b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n",
      " num_days_late | 348.6458333 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    num_days_late\n",
    "FROM case\n",
    "WHERE case_closed = false\n",
    "    AND case_late = true\n",
    "ORDER BY num_days_late DESC\n",
    "LIMIT 1;\n",
    "''').show(vertical = True, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e261ba-f41f-4058-87f5-23ccf928d6f3",
   "metadata": {},
   "source": [
    "How long has the oldest (in terms of days since opened) currently opened issue been open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4005ddd7-163b-45bb-b9cc-f6b0e70eb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " days_open | 1965 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    DATEDIFF(NOW(), case_opened_date) AS days_open\n",
    "FROM case\n",
    "WHERE case_closed = false\n",
    "ORDER BY days_open DESC\n",
    "LIMIT 1;\n",
    "''').show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7a62f-353a-4b07-8641-8afe1e038a0b",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "How many Stray Animal cases are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92c3fb-6476-4750-8396-e597a801eba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
